{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RodolfoFerro/ml-facilito/blob/main/notebooks/Modelos_Fundacionales_%26_Hugging_Face_Clase_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6jO_1gISKxk"
      },
      "source": [
        "# Modelos Fundacionales & Hugging Face - Clase 4  üß†\n",
        "\n",
        "> **Descripci√≥n:** Cuaderno de contenidos (IV) sobre Modelos Fundacionales & Hugging Face para el Bootcamp de Machine Learning con C√≥digo Facilito, 2023-2025. <br>\n",
        "> **Autor:** [Rodolfo Ferro](https://github.com/RodolfoFerro) <br>\n",
        "> **Contacto:** [X](https://twitter.com/rodo_ferro) / [Instagram](https://www.instagram.com/rodo_ferro/)\n",
        "\n",
        "\n",
        "## Contenido\n",
        "\n",
        "### Modelos fundacionales\n",
        "\n",
        "* ¬øQu√© son los modelos fundacionales?\n",
        "* Caracter√≠sticas de los modelos fundacionales\n",
        "* ¬øPor qu√© es importante el modelado fundacional?\n",
        "* ¬øC√≥mo funcionan los modelos fundacionales?\n",
        "* ¬øQu√© pueden hacer los modelos fundacionales?\n",
        "* Ejemplos de modelos fundacionales\n",
        "\n",
        "\n",
        "### Hugging Face - *Transformers*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNVG2PnSEtQN"
      },
      "source": [
        "## **Modelos fundacionales**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPk1Rkc4FZ5g"
      },
      "source": [
        "### **¬øQu√© son los modelos fundacionales?**\n",
        "\n",
        "Los **modelos fundacionales**, o FMs, son **redes neuronales de aprendizaje profundo** que han transformado el enfoque de los cient√≠ficos en el machine learning.\n",
        "\n",
        "*Estos modelos se entrenan con grandes conjuntos.*\n",
        "\n",
        "> **IDEA:** En lugar de desarrollar IA desde cero, las y los cient√≠ficos de datos utilizan un modelo fundacional como punto de partida para desarrollar modelos de ML que impulsen aplicaciones nuevas de manera r√°pida y rentable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Caracter√≠sticas de los modelos fundacionales**\n",
        "\n",
        "La **caracter√≠stica distintiva** de los modelos fundacionales es su **capacidad de adaptarse**. Estos modelos pueden realizar diversas tareas con alta precisi√≥n, como procesamiento de lenguaje natural, respuesta a preguntas y clasificaci√≥n robusta de im√°genes.\n",
        "\n",
        "El tama√±o y la versatilidad de los modelos fundamentales los distinguen de los modelos convencionales de aprendizaje autom√°tico, los cuales t√≠picamente se especializan en tareas particulares, como examinar texto para evaluar opiniones, categorizar im√°genes y prever patrones.\n"
      ],
      "metadata": {
        "id": "AN4GxfmiCnuz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **¬øPor qu√© es importante el modelado fundacional?**\n",
        "\n",
        "El modelado fundacional tiene el potencial de **revolucionar el ciclo de vida del machine learning**.\n",
        "\n",
        "Aunque desarrollarlos desde cero es costoso, utilizar modelos pre-entrenados resulta m√°s r√°pido y econ√≥mico a largo plazo, impulsando la automatizaci√≥n de diversas aplicaciones.\n",
        "\n",
        "Una aplicaci√≥n posible implica la automatizaci√≥n de diversas tareas y procesos, especialmente aquellas que demandan habilidades de razonamiento. Algunos ejemplos de √°reas donde los FMs podr√≠an utilizarse:\n",
        "\n",
        "* Asistencia al cliente\n",
        "* Traducci√≥n de idiomas\n",
        "* Generaci√≥n de contenido\n",
        "* Redacci√≥n de textos publicitarios\n",
        "* Clasificaci√≥n de im√°genes\n",
        "* Creaci√≥n y edici√≥n de im√°genes de alta resoluci√≥n\n",
        "* Extracci√≥n de documentos\n",
        "* Sector de la salud\n",
        "\n"
      ],
      "metadata": {
        "id": "OcSLY84GC2ab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **¬øC√≥mo funcionan los modelos fundacionales?**\n",
        "\n",
        "Los modelos fundacionales son una forma de **IA generativa**, operan mediante complejas redes neuronales, como las generativas antag√≥nicas/adversariales (GANs), los transformadores (Transformers) y los autoencoders variacionales.\n",
        "\n",
        "> ‚Üí Ellos generan resultados a partir de una o m√°s entradas (indicaciones) en forma de instrucciones en lenguaje humano.\n"
      ],
      "metadata": {
        "id": "mCbTCwZYDJRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **¬øQu√© pueden hacer los modelos fundacionales?**\n",
        "\n",
        "A pesar de ser pre-entrenados, los modelos fundacionales pueden continuar aprendiendo de nuevas entradas, realizando tareas como:\n",
        "\n",
        "* Procesamiento del lenguaje natural\n",
        "* Comprensi√≥n visual\n",
        "* Generaci√≥n de c√≥digo\n",
        "* Voz a texto\n",
        "* Participaci√≥n centrada en las personas\n"
      ],
      "metadata": {
        "id": "oogP9WMLDQ2G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Ejemplos de modelos fundacionales**\n",
        "\n",
        "La oferta de modelos fundacionales ha experimentado un r√°pido crecimiento en tama√±o y cantidad en el mercado. Algunos ejemplos notables incluyen:\n",
        "\n",
        "* BERT\n",
        "* GPTs\n",
        "* Stable Diffusion\n",
        "* BLOOM\n",
        "* Gemini‚Ä¶\n",
        "* ‚Ä¶y la plataforma Hugging Face.\n"
      ],
      "metadata": {
        "id": "MK354PrQDXpG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlQl_jAunKvb"
      },
      "source": [
        "## **Hugging Face - Transformers**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Hugging Face**\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\" width=\"30%\">\n",
        "</center>\n",
        "\n",
        "\n",
        "[Hugging Face](https://huggingface.co/) es una plataforma que ofrece herramientas open-source para la creaci√≥n e implementaci√≥n de modelos de ML. Funciona como un punto de encuentro comunitario donde las y los desarrolladores tienen la posibilidad de intercambiar y explorar modelos y conjuntos de datos.\n",
        "\n",
        "**La plataforma ofrece acceso p√∫blico a casi 200,000 modelos y 30,000 conjuntos de datos.**\n",
        "\n",
        "> La membres√≠a individual es de acceso gratuito, mientras que las\n",
        "suscripciones de pago brindan niveles de acceso m√°s avanzados.\n"
      ],
      "metadata": {
        "id": "fp687hVHnOxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **¬øQu√© es NLP?**\n",
        "\n",
        "El **Procesamiento del Lenguaje Natural** (PLN/NLP) es un campo de la ling√º√≠stica y el aprendizaje autom√°tico que se centra en comprender todo lo relacionado con el lenguaje humano. Su objetivo no es solo entender palabras individualmente, sino comprender el contexto en el que se utilizan.\n",
        "\n",
        "La siguiente es una lista de tareas comunes de procesamiento del lenguaje natural (NLP), con algunos ejemplos de cada una:\n",
        "\n",
        "* **Clasificaci√≥n de oraciones completas:** Obtener el sentimiento de una rese√±a, detectar si un correo electr√≥nico es spam, determinar si una oraci√≥n es gramaticalmente correcta o si dos oraciones est√°n l√≥gicamente relacionadas o no.\n",
        "* **Clasificaci√≥n de cada palabra en una oraci√≥n:** Identificar los componentes gramaticales de una oraci√≥n (sustantivo, verbo, adjetivo) o las entidades nombradas (persona, ubicaci√≥n, organizaci√≥n).\n",
        "* **Generaci√≥n de contenido de texto:** Completar un est√≠mulo con texto generado autom√°ticamente, llenar los espacios en blanco en un texto con palabras enmascaradas.\n",
        "* **Extracci√≥n de una respuesta de un texto:** Dada una pregunta y un contexto, extraer la respuesta a la pregunta bas√°ndose en la informaci√≥n proporcionada en el contexto.\n",
        "Generaci√≥n de una nueva oraci√≥n a partir de un texto de entrada: Traducir un texto a otro idioma, resumir un texto.\n",
        "\n",
        "#### **¬øPor qu√© es dif√≠cil?**\n",
        "\n",
        "Las computadoras no manejan ni procesan la informaci√≥n de la misma manera que lo hacemos los humanos.\n",
        "\n",
        "Para los modelos de ML, estas tareas son complicadas. Es necesario procesar el texto de manera que permita al modelo aprender de √©l.\n",
        "\n",
        "Dada la complejidad del lenguaje, es crucial reflexionar sobre c√≥mo llevar a cabo este procesamiento. Se ha llevado a cabo mucha investigaci√≥n sobre c√≥mo representar el texto."
      ],
      "metadata": {
        "id": "OHcha8EDD8vp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Transformers***\n",
        "\n",
        "Los modelos **Transformer** se emplean para abordar diversas tareas de procesamiento del lenguaje natural.\n",
        "\n",
        "Algunas empresas y organizaciones aprovechan Hugging Face y los modelos *Transformer*, devolviendo tambi√©n a la comunidad al compartier sus propios modelos.\n",
        "\n",
        "La biblioteca [ü§ó Transformers](https://github.com/huggingface/transformers) brinda la funcionalidad necesaria para crear y utilizar estos modelos compartidos.\n",
        "\n",
        "Adem√°s, el [Model Hub](https://huggingface.co/models) alberga miles de modelos preentrenados que est√°n disponibles para descargar y utilizar por cualquier persona."
      ],
      "metadata": {
        "id": "jIjoWdYOEK4J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para comenzar a usar transformers, la biblioteca Transformers cuenta con el elemento m√°s b√°sico, que es la funci√≥n `pipeline()`.\n",
        "\n",
        "Esta funci√≥n conecta un modelo con sus pasos de preprocesamiento y postprocesamiento necesarios, lo que nos permite ingresar directamente cualquier texto y obtener una respuesta comprensible."
      ],
      "metadata": {
        "id": "buDR65LWHLlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\"¬°Me encanta el bootcamp de ML de C√≥digo Facilito!\")"
      ],
      "metadata": {
        "id": "7TmaXov1G-hE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.model"
      ],
      "metadata": {
        "id": "zmAe4lwdgBYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier([\n",
        "    \"Feliz navidad para todos ustedes, jingle bells, jingle bells, jingle...\",\n",
        "    \"Hoy no me gusta c√≥mo est√° el clima.\"\n",
        "])"
      ],
      "metadata": {
        "id": "sEQWmPm7Hohj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "De manera predeterminada, este `pipeline` selecciona un modelo pre-entrenado espec√≠fico que ha sido ajustado para analizar sentimientos. El modelo se descarga y almacena en cach√© al crear el objeto clasificador.\n",
        "\n",
        "> **Nota:** Si vovlemos a ejecutar el comando, se utilizar√° el modelo almacenado en cach√©, evitando la necesidad de volver a descargarlo.\n",
        "\n",
        "Cuando ingresamos un texto en este pipeline, se llevan a cabo tres pasos esenciales:\n",
        "\n",
        "1. El texto se transforma previamente a un formato que el modelo pueda comprender.\n",
        "2. Las entradas preprocesadas se suministran al modelo.\n",
        "3. Las predicciones del modelo se procesan posteriormente para que tengan sentido.\n",
        "\n",
        "Algunos de los pipelines que est√°n disponibles en la actualidad incluyen:\n",
        "\n",
        "* `feature-extraction` (obtenci√≥n de la representaci√≥n vectorial de un texto)\n",
        "* `question-answering`\n",
        "* `sentiment-analysis` (an√°lisis de sentimientos)\n",
        "* `summarization` (resumen)\n",
        "* `text-generation` (generaci√≥n de texto)\n",
        "* `translation` (traducci√≥n)\n",
        "* `zero-shot-classification` (clasificaci√≥n sin etiquetas previas)"
      ],
      "metadata": {
        "id": "Mfza1FInIFHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Uso de Transormers**"
      ],
      "metadata": {
        "id": "DuiTEiokI4th"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Generaci√≥n de texto**"
      ],
      "metadata": {
        "id": "tBnZfa-tJlM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\")"
      ],
      "metadata": {
        "id": "41XWHMRaIxHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator(\"En este bootcamp sobre Machine Learning de C√≥digo Facilito aprender√°s\")"
      ],
      "metadata": {
        "id": "sRqbF7faJO5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator(\"In this bootcamp about ML, you'll learn\")"
      ],
      "metadata": {
        "id": "v2cfDRauJRvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos utilizar alg√∫n modelo espec√≠fico, simplemente yendo al [Model Hub](https://huggingface.co/models) y dando click en el tag correspondiente para desplegar modelos asociados a la tarea."
      ],
      "metadata": {
        "id": "1sPKuD8yJrAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\"text-generation\", model=\"distilgpt2\")"
      ],
      "metadata": {
        "id": "3zripUL5JthC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator(\n",
        "    \"In this bootcamp about Machine Learning, you'll learn\",\n",
        "    max_new_tokens=30,\n",
        "    num_return_sequences=2,\n",
        "    truncation=True\n",
        ")"
      ],
      "metadata": {
        "id": "8Hfl1J2_JyaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Resumen de contenido**"
      ],
      "metadata": {
        "id": "-QJaika1JpZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\")\n",
        "summarizer(\n",
        "    \"\"\"\n",
        "    La caracter√≠stica distintiva de los modelos fundacionales es su\n",
        "    capacidad de adaptarse. Estos modelos pueden realizar diversas\n",
        "    tareas con alta precisi√≥n, como procesamiento de lenguaje natural,\n",
        "    respuesta a preguntas y clasificaci√≥n robusta de im√°genes.\n",
        "\n",
        "    El tama√±o y la versatilidad de los modelos fundamentales los\n",
        "    distinguen de los modelos convencionales de aprendizaje autom√°tico,\n",
        "    los cuales t√≠picamente se especializan en tareas particulares, como\n",
        "    examinar texto para evaluar opiniones, categorizar im√°genes y prever\n",
        "    patrones.\n",
        "\"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "E_em1LP6MHYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Respuestas a preguntas**"
      ],
      "metadata": {
        "id": "uTwW4Y5VMude"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\")\n",
        "question_answerer(\n",
        "    question=\"¬øC√≥mo me llamo?\",\n",
        "    context=\"Me llamo Rodo. Nac√≠ en Guanajuato, M√©xico.\",\n",
        ")"
      ],
      "metadata": {
        "id": "9b5ZcMe4Mw-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_answerer(\n",
        "    question=\"Where am I from?\",\n",
        "    context=\"My name is Rodo. I was born in Guanajuato, M√©xico.\",\n",
        ")"
      ],
      "metadata": {
        "id": "DpSuwf1ENSiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Otras tareas**\n",
        "\n",
        "Puedes consultar otras tareas de ejemplo en esta [referencia](https://huggingface.co/learn/nlp-course/chapter1/3)."
      ],
      "metadata": {
        "id": "gLty2zBSOFgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **¬øC√≥mo funcionan?**\n",
        "\n",
        "Podr√≠amos decir que en general los Transformers se pueden agrupar en tres categor√≠as:\n",
        "\n",
        "* Estilo GPT (tambi√©n llamados modelos auto-regresivos Transformer)\n",
        "* Estilo BERT (tambi√©n llamados modelos auto-codificadores Transformer)\n",
        "* Estilo BART/T5 (tambi√©n llamados modelos Transformer de secuencia a secuencia)\n",
        "\n",
        "Todos los modelos Transformer mencionados anteriormente (GPT, BERT, BART, T5, etc.) han sido entrenados como modelos de lenguaje. Esto significa que han sido entrenados en grandes cantidades de texto sin procesar de manera auto supervisada.\n",
        "\n",
        "> El aprendizaje auto supervisado es un tipo de entrenamiento en el que el objetivo se calcula autom√°ticamente a partir de las entradas del modelo. ¬°Esto significa que no se necesita la intervenci√≥n humana para etiquetar los datos!\n",
        "\n",
        "Este tipo de modelos desarrollan una comprensi√≥n estad√≠stica del lenguaje en el que ha sido entrenado, pero no es muy √∫til para tareas pr√°cticas espec√≠ficas. Debido a esto, el modelo preentrenado general pasa por un proceso de transfer learning. Durante este proceso, el modelo se ajusta finamente de manera supervisada, es decir, utilizando etiquetas anotadas por humanos, en una tarea espec√≠fica."
      ],
      "metadata": {
        "id": "hCXtbQ6-b8Zy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un ejemplo de tarea es predecir la siguiente palabra en una oraci√≥n despu√©s de haber le√≠do las `n` palabras anteriores. Esto se llama modelado de lenguaje causal porque la salida depende de las entradas pasadas y presentes, pero no de las futuras.\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg\" width=\"80%\">\n",
        "</center>"
      ],
      "metadata": {
        "id": "BxM5lC6jckVq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Arquitectura general**\n",
        "\n",
        "El modelo est√° principalmente compuesto por dos bloques:\n",
        "\n",
        "* **Encoder:** El codificador recibe una entrada y construye una representaci√≥n de ella (sus caracter√≠sticas). Esto significa que el modelo est√° optimizado para adquirir comprensi√≥n a partir de la entrada.\n",
        "\n",
        "* **Decoder:** El decodificador utiliza la representaci√≥n del codificador (caracter√≠sticas) junto con otras entradas para generar una secuencia objetivo. Esto significa que el modelo est√° optimizado para generar salidas.\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg\" width=\"60%\">\n",
        "</center>\n",
        "\n",
        "Cada una de estas partes puede utilizarse de forma independiente seg√∫n la tarea:\n",
        "\n",
        "* **Modelos s√≥lo de codificaci√≥n:** √ötiles para tareas que requieren comprensi√≥n de la entrada, como la clasificaci√≥n de oraciones y el reconocimiento de entidades nombradas.\n",
        "* **Modelos s√≥lo de decodificaci√≥n:** Apropiados para tareas generativas como la generaci√≥n de texto.\n",
        "* **Modelos de codificaci√≥n-decodificaci√≥n o modelos de secuencia a secuencia:** Eficientes para tareas generativas que requieren una entrada, como la traducci√≥n o la s√≠ntesis."
      ],
      "metadata": {
        "id": "ijExO_ZkcyvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Capas de atenci√≥n**\n",
        "\n",
        "Una caracter√≠stica clave de los modelos Transformer es que est√°n construidos con capas especiales llamadas **capas de atenci√≥n**. De hecho, el t√≠tulo del art√≠culo que introduce la arquitectura Transformer es [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762) (\"La Atenci√≥n es Todo lo que Necesitas\"). Esta capa indicar√° al modelo que preste atenci√≥n espec√≠fica a ciertas palabras en la oraci√≥n que le pasamos (y m√°s o menos ignore las dem√°s) al tratar con la representaci√≥n de cada palabra.\n",
        "\n",
        "> Para contextualizar esto, considera la tarea de traducir texto de ingl√©s a franc√©s. Dada la entrada \"You like this course\" (\"Te gusta este curso\"), un modelo de traducci√≥n tambi√©n necesitar√° prestar atenci√≥n a la palabra adyacente \"You\" para obtener la traducci√≥n adecuada de la palabra \"like\", porque en franc√©s el verbo \"like\" se conjuga de manera diferente seg√∫n el sujeto. El resto de la oraci√≥n, sin embargo, no es √∫til para la traducci√≥n de esa palabra. De manera similar, al traducir \"this\" (\"esto\" o \"esta\"), el modelo tambi√©n deber√° prestar atenci√≥n a la palabra \"course\" (\"curso\"), porque \"this\" se traduce de manera diferente seg√∫n si el sustantivo asociado es masculino o femenino. Nuevamente, las otras palabras en la oraci√≥n no importar√°n para la traducci√≥n de \"this\". Con oraciones m√°s complejas (y reglas gramaticales m√°s complejas), el modelo necesitar√≠a prestar especial atenci√≥n a palabras que puedan aparecer m√°s lejos en la oraci√≥n para traducir correctamente cada palabra.\n",
        "\n",
        "El mismo concepto se aplica a cualquier tarea asociada con el lenguaje natural: una palabra por s√≠ sola tiene un significado, pero ese significado se ve profundamente afectado por el contexto, que puede ser cualquier otra palabra (o palabras) antes o despu√©s de la palabra que se est√° estudiando."
      ],
      "metadata": {
        "id": "ft_PVZNDdizB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **La arquitectura original**\n",
        "\n",
        "* La arquitectura Transformer fue dise√±ada originalmente para la traducci√≥n. Durante el entrenamiento, el codificador recibe entradas (oraciones) en un cierto idioma, mientras que el decodificador recibe las mismas oraciones en el idioma objetivo deseado.\n",
        "* En el codificador, las capas de atenci√≥n pueden usar todas las palabras en una oraci√≥n (ya que, como acabamos de ver, la traducci√≥n de una palabra dada puede depender de lo que haya despu√©s y antes en la oraci√≥n).\n",
        "* Sin embargo, el decodificador trabaja de manera secuencial y solo puede prestar atenci√≥n a las palabras en la oraci√≥n que ya ha traducido (es decir, solo las palabras antes de la palabra que se est√° generando en ese momento).\n",
        "\n",
        "Para acelerar las cosas durante el entrenamiento (cuando el modelo tiene acceso a las oraciones objetivo), se le proporciona al decodificador todo el objetivo, pero no se le permite usar palabras futuras. Por ejemplo, al intentar predecir la cuarta palabra, la capa de atenci√≥n solo tendr√° acceso a las palabras en las posiciones 1 a 3.\n",
        "\n",
        "La arquitectura original de Transformer se ve√≠a as√≠, con el codificador a la izquierda y el decodificador a la derecha:\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg\" width=\"90%\">\n",
        "</center>\n",
        "\n",
        "Observa que la primera capa de atenci√≥n en un bloque de decodificador presta atenci√≥n a todas las entradas (pasadas) al decodificador, pero la segunda capa de atenci√≥n utiliza la salida del codificador. Por lo tanto, puede acceder a toda la oraci√≥n de entrada para predecir mejor la palabra actual. Esto es muy √∫til, ya que diferentes idiomas pueden tener reglas gramaticales que colocan las palabras en diferentes √≥rdenes, o alg√∫n contexto proporcionado m√°s tarde en la oraci√≥n puede ser √∫til para determinar la mejor traducci√≥n de una palabra dada.\n",
        "\n",
        "La m√°scara de atenci√≥n tambi√©n se puede utilizar en el codificador/decodificador para evitar que el modelo preste atenci√≥n a algunas palabras especiales, como la palabra de relleno especial utilizada para igualar la longitud de todas las entradas al agrupar oraciones."
      ],
      "metadata": {
        "id": "EnqWmQ0leGp-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Entonces...**\n",
        "\n",
        "Estos elementos te resumen un poco el funcionamiento en alto nivel sobre este tipo de modelos:\n",
        "\n",
        "- Encoding posicional\n",
        "- Atenci√≥n\n",
        "    - Atenci√≥n propia (ahora con contexto sobre las palabras alrededor de la observada)"
      ],
      "metadata": {
        "id": "3r-2PGHLfbRW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Te recomiendo mucho ver estos videos:\n",
        "\n",
        "\n",
        "*   [¬øQu√© es un TRANSFORMER? La Red Neuronal que lo cambi√≥ TODO!](https://www.youtube.com/watch?v=aL-EmKuB078)\n",
        "*   [Transformers, explained: Understand the model behind GPT, BERT, and T5](https://www.youtube.com/watch?v=SZorAJ4I-sA)\n",
        "* [Illustrated Guide to Transformers Neural Network: A step by step explanation](https://www.youtube.com/watch?v=4Bdc55j80l8)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZU30p5IKezOP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Modelos y tokenizers**\n",
        "\n",
        "Como mencionamos, `pipeline` agrupa tres pasos: preprocesamiento de entradas, paso de las entradas a trav√©s del modelo y posprocesamiento.\n",
        "\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg\" width=\"90%\">\n",
        "</center>"
      ],
      "metadata": {
        "id": "zJEGZrJGOPZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Preprocesamiento con un tokenizer**\n",
        "\n",
        "Al igual que otras redes neuronales, los modelos Transformer no pueden procesar texto directamente, por lo que el primer paso de nuestro flujo (pipeline) es convertir las entradas de texto en n√∫meros comprensibles para el modelo. Para ello, utilizamos un tokenizador, que se encargar√° de:\n",
        "\n",
        "1. Dividir la entrada en palabras, subpalabras o s√≠mbolos (como la puntuaci√≥n) llamados tokens.\n",
        "2. Asignar un n√∫mero entero a cada token.\n",
        "3. Agregar entradas adicionales que puedan ser √∫tiles para el modelo.\n",
        "\n",
        "Todo este preprocesamiento debe realizarse de la misma manera que cuando el modelo fue pre-entrenado.\n"
      ],
      "metadata": {
        "id": "nywc9eYPUiay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\""
      ],
      "metadata": {
        "id": "t7vERWu6U0lF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "Ynn3xWJ3U4ZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_inputs = [\n",
        "    \"I hate when my neighbor does not feed her cat.\",\n",
        "    \"I enjoy coding in Python.\",\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(inputs)"
      ],
      "metadata": {
        "id": "6iXzIB6BU531"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La salida en s√≠ es un diccionario que contiene dos claves, `input_ids` y `attention_mask`. `input_ids` contiene dos tensores (de PyTorch) con n√∫meros enteros (una para cada oraci√≥n) que son los identificadores √∫nicos de los tokens en cada oraci√≥n.\n",
        "\n",
        "En cuanto a `attention_mask`, una m√°scara de atenci√≥n es una m√°scara binaria que designa qu√© tokens deben ser atendidos (asign√°ndoles pesos no nulos) y cu√°les deben ser ignorados (asign√°ndoles pesos nulos). Al aplicar esta m√°scara, el modelo puede atender selectivamente a tokens espec√≠ficos mientras ignora otros."
      ],
      "metadata": {
        "id": "5RNb-SZVVKUO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Procesamiento con el modelo**\n",
        "\n",
        "Ahora podemos descargar un modelo pre-entrenado de la misma manera que lo hicimos con nuestro tokenizador. Para ello, podemos usar el objeto `AutoModel` que tambi√©n tiene un m√©todo `from_pretrained()`:"
      ],
      "metadata": {
        "id": "aC2gRW89YBAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\""
      ],
      "metadata": {
        "id": "uyMDdIAcVbLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModel.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "X9HAiXdIViTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta arquitectura contiene solo el m√≥dulo base del Transformer: dados algunos inputs, produce lo que llamaremos estados ocultos, tambi√©n conocidos como caracter√≠sticas. Para cada input del modelo, recuperaremos un **vector de alta dimensionalidad** que representa la comprensi√≥n contextual de ese input por parte del modelo Transformer."
      ],
      "metadata": {
        "id": "TH__Ke9savGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(**inputs)\n",
        "print(outputs.last_hidden_state.shape)"
      ],
      "metadata": {
        "id": "9dvVlc3SVk2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **¬øUn vector de alta dimensionalidad?**\n",
        "\n",
        "El vector producido por el m√≥dulo Transformer suele ser grande. Generalmente tiene tres dimensiones:\n",
        "\n",
        "- **Tama√±o del batch:** El n√∫mero de secuencias procesadas a la vez (2 en nuestro ejemplo).\n",
        "- **Longitud de la secuencia:** La longitud de la representaci√≥n num√©rica de la secuencia (13 en este ejemplo).\n",
        "- **Tama√±o oculto:** La dimensi√≥n del vector de cada input del modelo.\n",
        "Se dice que es \"de alta dimensionalidad\" debido al √∫ltimo valor. El tama√±o oculto puede ser muy grande (768 es com√∫n para modelos m√°s peque√±os, y en modelos m√°s grandes esto puede alcanzar 3072 o m√°s)."
      ],
      "metadata": {
        "id": "LaMco-_5a9dC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Cabezas del modelo**\n",
        "\n",
        "Las cabezas del modelo toman el vector de alta dimensionalidad de estados ocultos como input y los proyectan a una dimensi√≥n diferente. Por lo general, est√°n compuestas por una o unas pocas capas lineales:\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg\" width=\"90%\">\n",
        "</center>\n",
        "\n",
        "La salida del modelo Transformer se env√≠a directamente a la cabeza del modelo para ser procesada.\n",
        "\n",
        "El modelo est√° representado por su capa de embeddings y las capas subsiguientes. La capa de embeddings convierte cada ID de input en el input tokenizado en un vector que representa el token asociado. Las capas subsiguientes manipulan esos vectores utilizando el mecanismo de atenci√≥n para producir la representaci√≥n final de las oraciones."
      ],
      "metadata": {
        "id": "KZoh6vIEbf4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hay muchas arquitecturas diferentes disponibles en Transformers, cada una dise√±ada para abordar una tarea espec√≠fica. Aqu√≠ hay una lista no exhaustiva:\n",
        "\n",
        "* Model\n",
        "* ForCausalLM\n",
        "* ForMaskedLM\n",
        "* ForMultipleChoice\n",
        "* ForQuestionAnswering\n",
        "* ForSequenceClassification\n",
        "* ForTokenClassification\n",
        "\n",
        "Para nuestro ejemplo, necesitaremos un modelo con una cabeza de clasificaci√≥n de secuencia (para poder clasificar las oraciones como positivas o negativas). Por lo tanto, en realidad, no usaremos la clase `AutoModel`, sino `AutoModelForSequenceClassification`:"
      ],
      "metadata": {
        "id": "qSfKVP-jgtzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\""
      ],
      "metadata": {
        "id": "yKVQn1lBVptm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "FfA-1gBUVrAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(**inputs)\n",
        "print(outputs.logits.shape)"
      ],
      "metadata": {
        "id": "M9CTkPT9Vr-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Pregunta:** ¬øPor qu√© es de 2x2?"
      ],
      "metadata": {
        "id": "Qeajh67NhZoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Postprocesaiento de la salida**"
      ],
      "metadata": {
        "id": "pJjYKjEJhh0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.logits)"
      ],
      "metadata": {
        "id": "7wRTwQtHVwmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "LCOGwS64V5bA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.id2label"
      ],
      "metadata": {
        "id": "ctQBoysNV4UM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Siguientes pasos...**\n",
        "\n",
        "- Preparaci√≥n de los datos\n",
        "- Fine-tuning de modelos:\n",
        "    - Con la API Trainer\n",
        "    - Con Keras\n",
        "\n",
        "Referencias: https://huggingface.co/learn/nlp-course/en/chapter0/1?fw=pt"
      ],
      "metadata": {
        "id": "QKp_PZ_NDqbS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------\n",
        "\n",
        "> Contenido creado por **Rodolfo Ferro**, 2023-2025. <br>\n",
        "> Puedes contactarme a trav√©s de Insta ([@rodo_ferro](https://www.instagram.com/rodo_ferro/)) o X ([@rodo_ferro](https://twitter.com/rodo_ferro))."
      ],
      "metadata": {
        "id": "hSdbQU3e6-Ky"
      }
    }
  ]
}